# SensatUrban dataset for 3D semantic segmentation

## Dataset preparation

Due to the large size of a single file of the SensatUrban dataset and can't train directly, we provide the following methods for generating datasets.
Please select the appropriate method according to your needs:

- 3D cropped dataset

  - [x] Type 1: square sliding window
  - [x] Type 2: square random  window
  - [ ] Type 3: random KNN

- 3D downsample dataset

  - [x] Type 1: random
  - [x] Type 2: uniform
  - [ ] Type 3: grid (not support yet, please refer to [official pipeline](https://github.com/QingyongHu/SensatUrban/blob/master/input_preparation.py))

Considering multimodal fusion or other uses, we provide methods to generate 2D semantic
segmentation dataset based on **3D square sliding or random dataset** .

- 2D semantic segmentation dataset (only support 3D square sliding or random dataset)

  - [x] Type 1: Depth image dataset
  - [x] Type 2: RGB image dataset

The above dataset types can be generated by any combination of settings,
and the following file structure organization styles are supported.

- [x] Style 1: Potsdam

- [ ] Style 2: SemanticKITTI

## Prepare SensatUrban dataset

Download and unzip `data_release.zip` then please place it in the
corresponding directory according to the following structure.

```
mmdetection3d
├── mmdet3d
├── tools
├── configs
├── data
│   ├── sensaturban
│   │   ├── train
│   │   │   ├── birmingham_block_0.ply
│   │   │   ├── xxxxx_block_x.ply
│   │   ├── test
│   │   │   ├── birmingham_block_2.ply
│   │   │   ├── xxxxx_block_x.ply
```

SensatUrban dataset has 37 train files and 6 test files,
and each ply file has`[x,y,z,red,green,blue,class]` data fields,
a total of 13 categories, they are:

`0: 'Ground', 1: 'High Vegetation', 2: 'Buildings', 3: 'Walls', 4: 'Bridge', 5: 'Parking', 6: 'Rail', 7: 'traffic Roads', 8: 'Street Furniture',  9: 'Cars', 10: 'Footpath', 11: 'Bikes', 12: 'Water'`

## Create dataset

Due to the problem of python multithreading, it is recommended to use the following methods to process data to speed up the processing speed.
First create a `create_sensaturban_dataset.py`

```python
import argparse
from sensaturban_converter import SensatUrbanConverter
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--id', default=0, type=int)
    converter = SensatUrbanConverter(
        root_path, # the path to the original dataset
        info_prefix, # prefix for info files
        out_dir, # the path to the output dataset
        workers, # Number of threads processing datasets concurrently
        to_image=False, # Whether to generate a 2D dataset, if true, the slicing method must be specified
        subsample_method='none', # Whether and how to generate a downsampled dataset
        crop_method='sliding', # Whether to generate tiled datasets and how to generate tiled datasets
        crop_size=12.5, # The side length of the sliced dataset is 2*crop_size, used with crop_method
        crop_scale=0.05, # In a 2D dataset, how many meters does each pixel represent
        subsample_rate=0.5, # Parameters in the downsampled dataset, the input is points when downsampling at random, and voxel size when downsampling voxels
        random_crop_ratio=1.0, # In the random slicing mode, the number of cuts is calculated according to the file size, and the default is random_crop_ratio times per MB.
    )
    args, opts = parser.parse_known_args()
    converter._convert2potsdam_one(args.id)
```

Then you can perform parallel processing by executing the following commands, please select the number of simultaneous processing according to the configuration of the running device.

```shell
python create_sensaturban_dataset.py --id 0 &
python create_sensaturban_dataset.py --id 1 &
...
```

Similar to other datasets, we can also generate use`python tools/create_data.py sensaturban --root-path ./data/sensaturban --out-dir ./data/sensaturban`,
But before that, we need to modify the parameters in the `create_data.py` file according to our own needs. In order to better understand how to generate a dataset, the following briefly introduces the parameters that affect the generated dataset:

```python
def sensaturban_data_prep(root_path,
                    info_prefix,
                    out_dir,
                    workers,
                    dataset_style='potsdam'): # The file organization style of the dataset

    converter = SensatUrbanConverter(
        root_path, # the path to the original dataset
        info_prefix, # prefix for info files
        out_dir, # the path to the output dataset
        workers, # Number of threads processing datasets concurrently
        to_image=False, # Whether to generate a 2D dataset, if true, the slicing method must be specified
        subsample_method='none', # Whether and how to generate a downsampled dataset
        crop_method='sliding', # Whether to generate tiled datasets and how to generate tiled datasets
        crop_size=12.5, # The side length of the sliced dataset is 2*crop_size, used with crop_method
        crop_scale=0.05, # In a 2D dataset, how many meters does each pixel represent
        subsample_rate=0.5, # Parameters in the downsampled dataset, the input is points when downsampling at random, and voxel size when downsampling voxels
        random_crop_ratio=1.0, # In the random slicing mode, the number of cuts is calculated according to the file size, and the default is random_crop_ratio times per MB.
    )
```

The following will provide some specific examples, you can choose according to your needs:

### 3D cropped dataset

- Type 1: square sliding window

```python
def sensaturban_data_prep(root_path,
                    info_prefix,
                    out_dir,
                    workers,
                    dataset_style='potsdam'): # The file organization style of the dataset

    converter = SensatUrbanConverter(
        root_path, # the path to the original dataset
        info_prefix, # prefix for info files
        out_dir, # the path to the output dataset
        workers, # Number of threads processing datasets concurrently
        to_image=False, # Does not generate 2D datasets
        subsample_method='none', # Do not generate downsampled datasets
        crop_method='sliding', # Sliced dataset using sliding window approach
        crop_size=12.5, # Each slice has a side length of 25m x 25m
    )
```

Now the output directory structure is:

```
data
├── sensaturban
│   ├── train
│   │   ├── points
│   │   │   ├── xxxxx.bin # point cloud file
│   │   ├── labels
│   │   │   ├── xxxxx.labels # The point label file corresponding to the point cloud
```

- Random crop method

```python
def sensaturban_data_prep(root_path,
                    info_prefix,
                    out_dir,
                    workers,
                    dataset_style='potsdam'): # The file organization style of the dataset

    converter = SensatUrbanConverter(
        root_path, # the path to the original dataset
        info_prefix, # prefix for info files
        out_dir, # The path to the output dataset
        workers, # Number of threads processing datasets concurrently
        to_image=False, # Does not generate 2D datasets
        subsample_method='none', # Do not generate downsampled datasets
        crop_method='random', # Sliced dataset using random slicing
        random_crop_ratio=1.0, # Slices every 1MB in a file
    )
```

Now the output directory structure is:

```
data
├── sensaturban
│   ├── train
│   │   ├── points
│   │   │   ├── xxxxx.bin # point cloud file
│   │   ├── labels
│   │   │   ├── xxxxx.labels # The point label file corresponding to the point cloud
```

### Downsample dataset

- Type 1: random

```python
def sensaturban_data_prep(root_path,
                    info_prefix,
                    out_dir,
                    workers,
                    dataset_style='potsdam'): # The file organization style of the dataset

    converter = SensatUrbanConverter(
        root_path, # the path to the original dataset
        info_prefix, # prefix for info files
        out_dir, # The path to the output dataset
        workers, # Number of threads processing datasets concurrently
        to_image=False, # Does not generate 2D datasets
        subsample_method='random', # Random downsampling
        crop_method='none', # Do not generate tiled datasets
        subsample_rate=100000, # Pick 100,000 points at random
    )
```

Now the output directory structure is:

```
data
├── sensaturban
│   ├── train
│   │   ├── points
│   │   │   ├── xxxxx.bin # point cloud file
│   │   ├── labels
│   │   │   ├── xxxxx.labels # The point label file corresponding to the point cloud
```

### 3D Square sliding window dataset & 2D semantic segmentation dataset

```python
def sensaturban_data_prep(root_path,
                    info_prefix,
                    out_dir,
                    workers,
                    dataset_style='potsdam'): # The file organization style of the dataset

    converter = SensatUrbanConverter(
        root_path, # the path to the original dataset
        info_prefix, # prefix for info files
        out_dir, # The path to the output dataset
        workers, # Number of threads processing datasets concurrently
        to_image=True, # Generating 2D Semantic Segmentation Datasets
        subsample_method='none', # Do not generate downsampled datasets
        crop_method='sliding', # Generate datasets by sliding windows
        crop_size=12.5, # Slice point cloud size 25m x 25m
        crop_scale=0.05, # Each pixel represents 0.05m, so the image size is 500x500
    )
```

Now the output directory structure is:

```
data
├── sensaturban
│   ├── train
│   │   ├── points
│   │   │   ├── xxxxx.bin # point cloud file
│   │   ├── rgbs
│   │   │   ├── xxxxx.png # rgb image file
│   │   ├── depths
│   │   │   ├── xxxxx.npy # depth map file
│   │   ├── masks
│   │   │   ├── xxxxx.png # 2D segmentation ground truth image
│   │   ├── labels
│   │   │   ├── xxxxx.labels # The point label file corresponding to the point cloud
```

## Training pipeline

```python
train_pipeline = [
        dict(
            type='LoadPointsFromFile',
            coord_type='LIDAR',
            shift_height=True,
            load_dim=6,
            use_dim=[0, 1, 2, 3, 4, 5]),
        dict(
            type='LoadImageFromFile',
            color_type='color',
            imdecode_backend='cv2'),
        dict(
            type='LoadDepthFromFile'),
        dict(
            type='LoadAnnotations3D',
            with_bbox_3d=False,
            with_label_3d=False,
            with_mask_3d=False,
            with_seg_3d=True,
            with_seg=True,
            seg_3d_dtype=np.int8),
        dict(
            type='Pack3DDetInputs',
            keys=[
                'points', 'img', 'depth_img', 'pts_semantic_mask', 'gt_seg_map'
            ])
        ]
```

## Metrics

Usually we use mean Intersection over Union (mIoU) as a metric for ScanNet semantic segmentation tasks.
Specifically, we first calculate the IoU for all classes, and then take the average as mIoU.
For more implementation details, please refer to [seg_eval.py](https://github.com/open-mmlab/mmdetection3d/blob/master/mmdet3d/core/evaluation/seg_eval.py).

In addition, a tool class `SensatUrbanEvaluator` for projecting from 2D datasets back to 3D datasets is provided. Due to the problems of python multithreading,
We propose to perform parallel processing in the following ways to speed up the reprojection process,
You can do this by creating a new `reproject.py` file and adding the following code and adjusting the parameter settings according to your dataset:

```python
import argparse
from sensaturban_data_utils import SensatUrbanEvaluator
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--id', default=0, type=int)
    evaluater = SensatUrbanEvaluator(
        split='test',
        dataset_path='./sensaturban',
        pred_path='./pred',
        crop_method='random',
        out_path='./ply_out',
        crop_size=12.5,
        bev_size=500,
        bev_scale=0.05,
        out_ply=False,
        out_label=True)
    args, opts = parser.parse_known_args()
    evaluater.generate(args.id)
```

Finally by executing the commands to process reprojection in parallel, note that random reprojection requires a large amount of memory. Please choose how many processes to execute according to the operating device.

```shell
python reproject.py --id 0 &
python reproject.py --id 1 &
python reproject.py --id 2 &
```
